---
layout: post
title: "Momentum Markowitz SIT"
date: "June 04, 2015"
---
```{r global_options, include=FALSE}
library(knitr)
require(PerformanceAnalytics)
require(TTR)
opts_chunk$set(tidy=FALSE, size='scriptsize',
               fig.align='center',  fig.width=7,  fig.height=5, 
               message=FALSE,warning=FALSE, fig.path=paste0(VAR, 'public/images/2015-06-04-Momentum_Markowitz/'))
options(width=81,continue=" ",digits=8)
```

```{r echo=FALSE}
function.name <- "Momentum_Markowitz_Post.R"
src.file.name <- file.path(
  getwd(), 
  "files", 
  "src", 
  paste("SRC_", function.name, sep="")
)
source(src.file.name)
rm(function.name, src.file.name)
```

The [Momentum and Markowitz: A Golden Combination (2015) by Keller, Butler, Kipnis](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2606884) paper is a review of practitioner's tools to make mean variance optimization portfolio a viable solution. In particular, authors suggest and test:

* adding maximum weight limits and
* adding target volatility constraint

to control solution of mean variance optimization.

Below I will have a look at the results for the 8 asset universe:

* S&P 500
* EAFE
* Emerging Markets
* US Technology Sector
* Japanese Equities
* 10-Year Treasuries
* T-Bills
* High Yield Bonds

First, let's load historical data for all assets

```{r}
load.packages('quantmod')

# load saved Proxies Raw Data, data.proxy.raw
# please see http://systematicinvestor.github.io/Data-Proxy/ for more details
load('files/data/data.proxy.raw.Rdata')

N8.tickers = '
US.EQ = VTI + VTSMX + VFINX
EAFE = EFA + VGTSX
EMER.EQ = EEM + VEIEX
TECH.EQ = QQQ + ^NDX
JAPAN.EQ = EWJ + FJPNX
MID.TR = IEF + VFITX
US.CASH = BIL + TB3M,
US.HY = HYG + VWEHX
'

data = env()

getSymbols.extra(N8.tickers, src = 'yahoo', from = '1970-01-01', env = data, raw.data = data.proxy.raw, set.symbolnames = T, auto.assign = T)

for(i in data$symbolnames) data[[i]] = adjustOHLC(data[[i]], use.Adjusted=T)

bt.prep(data, align='remove.na', fill.gaps = T)
```

Next, let's test the functionality of kellerCLAfun from Appendix A

```{r}
#*****************************************************************
# Run tests, monthly data - works
#*****************************************************************
prices = data$prices
period.ends<-endpoints(prices, "months")
period.ends<-period.ends[-1]
period.ends<-period.ends[-length(period.ends)]
n<-ncol(prices)
data = bt.change.periodicity(data, periodicity = 'months')

plota.matplot(scale.one(data$prices))

res = kellerCLAfun(prices[period.ends-1,], returnWeights = T, 0.25, 0.1, c('US.CASH', 'MID.TR'))

plotbt.transition.map(res[[1]]['2013::'])

plota(cumprod(1 + res[[2]]), type='l')
```

Next, let's create a benchmark and set up commision structure to be used for all tests.

```{r}
#*****************************************************************
# Create a benchmark
#*****************************************************************
models = list()  

commission = list(cps = 0.01, fixed = 10.0, percentage = 0.0)

data$weight[] = NA
	data$weight$US.EQ = 1
	data$weight[1:12,] = NA
models$US.EQ = bt.run.share(data, clean.signal=T, commission=commission, trade.summary=T, silent=T)
```

Next, let's take weights from the kellerCLAfun and use them to create a back-test

```{r}
#*****************************************************************
# transform kellerCLAfun into model results
#*****************************************************************
obj = list(weights = list(CLA = res[[1]]), period.ends = index(prices[period.ends,])[-1:-12])
models = c(models, create.strategies(obj, data, commission=commission, trade.summary=T, silent=T)$models)
```

We can easily replicate same results with base SIT functionality

```{r}
#*****************************************************************
# Replicate using base SIT functionality
#*****************************************************************
weight.limit = data.frame(last(prices))
  weight.limit[] = 0.25
	weight.limit$US.CASH = weight.limit$MID.TR = 1

obj = portfolio.allocation.helper(prices[period.ends-1,], 
	periodicity = 'months', lookback.len = 12, silent=T, 
		const.ub = weight.limit,
		create.ia.fn = 	function(hist.returns, index, nperiod) {
			ia = create.ia(hist.returns, index, nperiod)
			ia$expected.return = (last(hist.returns,1) + colSums(last(hist.returns,3)) + 
				colSums(last(hist.returns,6)) + colSums(last(hist.returns,12))) / 22
			ia
		},
		min.risk.fns = list(
			TRISK = target.risk.portfolio(target.risk = 0.1, annual.factor=12)
		)
	)
	
models = c(models, create.strategies(obj, data, commission=commission, trade.summary=T, silent=T)$models)
```

Another idea is to use Pierre Chretien's Averaged Input Assumptions

```{r}
#*****************************************************************
# Let's use Pierre's Averaged Input Assumptions 
#*****************************************************************
obj = portfolio.allocation.helper(prices[period.ends-1,], 
  periodicity = 'months', lookback.len = 12, silent=T, 
		const.ub = weight.limit,
		create.ia.fn = 	create.ia.averaged(c(1,3,6,12), 0),
		min.risk.fns = list(
			TRISK.AVG = target.risk.portfolio(target.risk = 0.1, annual.factor=12)
		)
	)

models = c(models, create.strategies(obj, data, commission=commission, trade.summary=T, silent=T)$models)


```

Finally we are ready to look at the results

```{r}
models = bt.trim(models)

plotbt(models, plotX = T, log = 'y', LeftMargin = 3, main = NULL)
  mtext('Cumulative Performance', side = 2, line = 1)

plotbt.strategy.sidebyside(models, make.plot=T, return.table=F, perfromance.fn = engineering.returns.kpi)

layout(1)
barplot.with.labels(sapply(models, compute.turnover, data), 'Average Annual Portfolio Turnover')
```

Our replication results are almost identical results to the results using kellerCLAfun.

Using Averaged Input Assumptions produces slightly better results.

The main point that a reader should remember from reading Momentum and Markowitz: A Golden Combination (2015) by Keller, Butler, Kipnis paper is that it is a bad idea to blindly use the optimizer. Instead, you should apply common sense heuristics mentioned in the paper to make solution robust across time and various universes.